{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1469fbfb",
   "metadata": {},
   "source": [
    "# LIA subsets\n",
    "\n",
    "LIA was divided into train, dev and test in [UD](https://github.com/UniversalDependencies/UD_Norwegian-NynorskLIA/tree/8a4ea1a6e0e1fbb4ef5ba34c2d408563e9c8cf9a).\n",
    "\n",
    "The [LIA](https://github.com/textlab/spoken_norwegian_resources/tree/master/treebanks/Norwegian-NynorskLIA) conllu-treebank is divided into 18 files, 1 per speaker/conversation.\n",
    "\n",
    "Each sentence has a unique `sent_id` across all partitions in UD, as opposed to LIA where each sentence is given a file-internal `id` which is incremental from 1 in each file. \n",
    "\n",
    "Here we map the speaker/file-id from UD + sent_id back to LIA to recreate the partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55150fa3",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import conllu\n",
    "from conllu import parse\n",
    "\n",
    "UD_path = Path(\"../data/UD_Norwegian-NynorskLIA\")\n",
    "LIA_path = Path(\"../spoken_norwegian_resources/treebanks/Norwegian-NynorskLIA\")\n",
    "LIA_old_path = Path(\"../spoken_norwegian_resources/treebanks/Norwegian-NynorskLIA_old\")\n",
    "\n",
    "\n",
    "def load_partition(filepath: Path, partition: str = \"train\") -> list:\n",
    "    \"\"\"Load one of the UD dataset partitions train, dev, or test.\"\"\"\n",
    "    data = next(filepath.glob(f\"*{partition}.conllu\")).read_text()\n",
    "    sentences = parse(\n",
    "        data,\n",
    "        metadata_parsers={\n",
    "            \"sent_id\": lambda key, value: (key, value),\n",
    "            \"text\": lambda key, value: (key, value),\n",
    "            \"__fallback__\": lambda key, value: [\n",
    "                [k.rstrip(\":\"), key.split()[i + 1]]\n",
    "                for i, k in list(enumerate(key.split()))[::2]\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_lia_sentences(\n",
    "    filestem: str,\n",
    "    dir_path: Path = Path(\"../spoken_norwegian_resources/treebanks/\"),\n",
    "    use_old: bool = False,\n",
    "):\n",
    "    LIA_path = dir_path / \"Norwegian-NynorskLIA\"\n",
    "    LIA_old_path = dir_path / \"Norwegian-NynorskLIA_old\"\n",
    "    filename = filestem + \".conll\"\n",
    "\n",
    "    lia_file = LIA_path / filename\n",
    "    # Shortcut to the old version if use_old is True\n",
    "    # otherwise use it as fallback\n",
    "    lia_file = LIA_path / filename if not use_old else LIA_old_path / filename\n",
    "    try:\n",
    "        lia_data = lia_file.read_text()\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            lia_file = LIA_old_path / filename\n",
    "            lia_data = lia_file.read_text()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Couldn't load {filestem}\")\n",
    "            lia_data = \"\"\n",
    "    finally:\n",
    "        lia_sentences = parse(lia_data)\n",
    "    return lia_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = \"dev\"\n",
    "(UD_sentences := load_partition(UD_path, partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff30b8",
   "metadata": {},
   "source": [
    "# Mapping \n",
    "Iterate over UD sentences and map them to the corresponding LIA sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ba62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ud_partition_to_lia_sentences(sentences: conllu.models.TokenList) -> dict:\n",
    "    mapping = {}\n",
    "    no_match = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_id = sentence.metadata[\"sent_id\"]\n",
    "        UD_text = sentence.metadata[\"text\"]\n",
    "        filestem = sentence.metadata[\"speakerid\"]\n",
    "        lia_sentences = load_lia_sentences(filestem)\n",
    "        for sent in lia_sentences:\n",
    "            LIA_text = sent.metadata[\"text\"]\n",
    "            if (LIA_text == UD_text) or (LIA_text == UD_text.rstrip(\" .\")):\n",
    "                mapping[sent_id] = sent\n",
    "\n",
    "        if sent_id not in mapping:\n",
    "            no_match[sent_id] = sentence\n",
    "\n",
    "    return {\"match\": mapping, \"no_match\": no_match}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = map_ud_partition_to_lia_sentences(UD_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b9a56",
   "metadata": {},
   "source": [
    "# Annotate the partition with correct sent_ids and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad338e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lia_partition = []\n",
    "for sent_id, sentence in mapping[\"match\"].items():\n",
    "    sentence.metadata[\"sent_id\"] = sent_id\n",
    "    lia_partition.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "def save_conllu(data, filepath: Path | str):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.writelines([sentence.serialize() for sentence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9941f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_LIA_UD_file = Path(f\"../data/lia_{partition}.conllu\")\n",
    "save_conllu(lia_partition, new_LIA_UD_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a5b52",
   "metadata": {},
   "source": [
    "## Handle mis-matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97670194",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_match = mapping[\"no_match\"]\n",
    "print(f\"No LIA sentence was found for {len(no_match)} UD {partition} sent_ids.\")\n",
    "\n",
    "no_match_fname = f\"no_match_{partition}.txt\"\n",
    "\n",
    "with open(no_match_fname, \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(no_match.keys()))\n",
    "\n",
    "print(f'They have been saved to \"{no_match_fname}\" for later processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33df9b",
   "metadata": {},
   "source": [
    "## Find missing sentences\n",
    "\n",
    "There is a mismatch between the id's in two versions of LIA: \n",
    "\n",
    "- The old version of the Vardø treebank has 9848 lines (spoken_norwegian_resources/treebanks/Norwegian-NynorskLIA_old/vardoe_uio_01.conll)\n",
    "- The new version has only 8838 lines\n",
    "\n",
    "---\n",
    "\n",
    "Sometimes the wrong sentence has been saved to the new LIA partition because of multiple instances of the same text (often \"ja\", \"jaha\", \"...\", etc.). It is easy to spot these cases when the incremental id numbers suddenly jump over a bunch of numbers before going back to its incremental series, e.g. \"id = 183\",  \"id= 660\" then \"id = 185\", etc. \n",
    "\n",
    "In these cases, just delete the wrong sentences from the `lia_{partition}.conllu` file and find them again with the troubleshooting cells here.\n",
    "\n",
    "---\n",
    "\n",
    "**NB**: Sentences that are just punctuation will be skipped:  `# text = - … .`\n",
    "\n",
    "This is the case for 3 sentences in `austevoll_uib_01.conll`\n",
    "- 003752\n",
    "- 003773\n",
    "- 003778\n",
    "- 005026 --> This one is a merge of `flakstad_uib_04` id `235` (\"ja\") and `236` (\"...\")\n",
    "- 001476 --> This one no longer exists in the new version of LIA, but had the id 282 in the old version. Would have been between ids 271 and 272 in the new LIA. \n",
    "- > TODO: Note that these sentences are skipped in the README when we publish LIA again\n",
    "\n",
    "--- \n",
    "\n",
    "There is one instance of two sentences being merged in the old UD LIA: \n",
    "\n",
    "UD `sent_id = 004443` corresponds to LIA `austevoll_uib_04.conll` id `80` and `81`. \n",
    "\n",
    "Instead of re-indexing all of UD and give new sentence ID's to all following LIA segments, we remove the segment before (which is a single word, \"nei\", and which there are more examples of elsewhere in the treebank) to reuse its sent_id `004442`. \n",
    "\n",
    "---\n",
    "\n",
    "Number of segments in each partition after I've found the missing ones: \n",
    "- LIA dev: 878\n",
    "- LIA test: 956\n",
    "- LIA train: 3411\n",
    "- **Total: 5245**\n",
    "\n",
    "In comparison, the old UD LIA treebank contains 5250 segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afa41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_segments(partition: str) -> int:\n",
    "    new_LIA_UD_file = Path(f\"../data/lia_{partition}.conllu\")\n",
    "    lia_partition = parse(new_LIA_UD_file.read_text())\n",
    "    indexed_LIA = {s.metadata[\"sent_id\"]: s for s in lia_partition}\n",
    "    print(f\"LIA {partition}: {len(indexed_LIA)}\")\n",
    "    return len(indexed_LIA)\n",
    "\n",
    "\n",
    "segments = 0\n",
    "for partition in (\"dev\", \"test\", \"train\"):\n",
    "    segments += count_segments(partition)\n",
    "\n",
    "print(f\"Total: {segments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d42b80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the old LIA UD partition and the new LIA UD partition\n",
    "partition = \"train\"\n",
    "ud_partition = load_partition(UD_path, partition=partition)\n",
    "new_LIA_UD_file = Path(f\"../data/lia_{partition}.conllu\")\n",
    "lia_partition = parse(new_LIA_UD_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f1d3b",
   "metadata": {},
   "source": [
    "### Index the treebanks with sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index and filter the two treebanks to get ids for the missing sentences\n",
    "indexed_UD = {s.metadata[\"sent_id\"]: s for s in ud_partition}\n",
    "print(f\"length for UD {partition}: {len(indexed_UD)}\")\n",
    "\n",
    "indexed_LIA = {s.metadata[\"sent_id\"]: s for s in lia_partition}\n",
    "partition_order = [int(sent_id) for sent_id in indexed_LIA.keys()]\n",
    "print(f\"length for LIA {partition}: {len(lia_partition)}\")\n",
    "\n",
    "missing_sents = set(indexed_UD.keys()).difference(set(indexed_LIA))\n",
    "missing = {\n",
    "    sent_id: indexed_UD[sent_id]\n",
    "    for sent_id in missing_sents\n",
    "    if not indexed_UD[sent_id].metadata[\"text\"] == \"- … .\"\n",
    "}  # Filter out the segments without any syntactic or semantic information\n",
    "missing_sents = sorted(list(missing.keys()), key=int)\n",
    "\n",
    "found = {}\n",
    "\n",
    "print(f\"{len(missing)} sentences in the UD {partition} were not found in LIA: \")\n",
    "for s in missing.values():\n",
    "    print(s.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8c14f",
   "metadata": {},
   "source": [
    "### Fetch missing UD sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one of the sentences from the old UD\n",
    "from pprint import pprint\n",
    "\n",
    "sent_id = missing_sents[0]\n",
    "sent_number = int(sent_id)\n",
    "UD_sentence = missing[sent_id]\n",
    "pprint(UD_sentence.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0c80b",
   "metadata": {},
   "source": [
    "\n",
    "### Find the corresponding sentence in LIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "filestem = UD_sentence.metadata[\"speakerid\"]\n",
    "# filestem_map = {\"lierne_uio_01\": \"nordli_uio_01\"} # Map some of the filestems from the old to the new\n",
    "# filestem = filestem_map.get(filestem, filestem)\n",
    "LIA_sentences = load_lia_sentences(filestem, use_old=False)  ### CHANGE `use_old`\n",
    "# if the resulting sentence doesn't match the previous cell's output\n",
    "\n",
    "before = str(sent_number - 1).zfill(6)\n",
    "after = str(sent_number + 1).zfill(6)\n",
    "\n",
    "context_id = before  ### CHANGE before / after\n",
    "context_sentence = indexed_LIA[context_id]\n",
    "lia_id = context_sentence.metadata[\"id\"]\n",
    "\n",
    "LIA_sent = LIA_sentences[int(lia_id)]  ## CHANGE Add / subtract to get the right index\n",
    "print(\n",
    "    (\n",
    "        f\"Sentence before/after the missing one ({sent_id}), \"\n",
    "        f\"with lia id {lia_id} and UD sent_id {context_sentence.metadata['sent_id']}: \"\n",
    "    )\n",
    ")\n",
    "print(context_sentence.metadata)\n",
    "print(f\"Sentence from {filestem} with the corresponding index number: \")\n",
    "print(LIA_sent.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf159f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\+\\s+\"\n",
    "# ,\n",
    "# \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899bf83",
   "metadata": {},
   "source": [
    "### Check for duplicate ids in the LIA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([sent.metadata[\"id\"] for sent in LIA_sentences])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f33d8",
   "metadata": {},
   "source": [
    "## Add found sentence to the LIA partition and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the correct sentence id and save it to the \"found\" dict\n",
    "LIA_sent.metadata[\"sent_id\"] = sent_id\n",
    "found[sent_id] = LIA_sent\n",
    "\n",
    "# insert the missing sentence at the right place\n",
    "if sent_number not in partition_order:\n",
    "    partition_order.append(sent_number)\n",
    "partition_order = sorted(partition_order)\n",
    "idx = partition_order.index(sent_number)\n",
    "if lia_partition[idx].metadata[\"sent_id\"] != sent_id:\n",
    "    lia_partition.insert(idx, found[sent_id])\n",
    "print(lia_partition[idx].metadata)\n",
    "\n",
    "# Remove from \"missing\"\n",
    "for sent_id in found:\n",
    "    if sent_id in missing:\n",
    "        del missing[sent_id]\n",
    "        missing_sents.remove(sent_id)\n",
    "\n",
    "print(f\"Still missing {len(missing)}: {[k for k in missing.keys()]}\")\n",
    "\n",
    "# Save to file\n",
    "print(f\"LIA {partition} length: {len(lia_partition)}\")\n",
    "save_conllu(lia_partition, new_LIA_UD_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grew_ndt2ud-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
